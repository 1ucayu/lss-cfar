{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSS-CFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## third-party implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from loguru import logger\n",
    "\n",
    "def triangular_toeplitz_multiply(u, v):\n",
    "    n = u.shape[-1]\n",
    "    u_expand = F.pad(u, (0, n))\n",
    "    v_expand = F.pad(v, (0, n))\n",
    "    u_f = torch.fft.rfft(u_expand, n=2*n, dim=-1)\n",
    "    v_f = torch.fft.rfft(v_expand, n=2*n, dim=-1)\n",
    "    uv_f = u_f * v_f\n",
    "    output = torch.fft.irfft(uv_f, n=2*n, dim=-1)[..., :n]\n",
    "    return output\n",
    "\n",
    "def krylov(L, A, b):\n",
    "    \"\"\" Compute the Krylov matrix (b, Ab, A^2b, ...) using the squaring trick.  \"\"\"\n",
    "\n",
    "    x = b.unsqueeze(-1) # (..., N, 1)\n",
    "    A_ = A\n",
    "\n",
    "    done = L == 1\n",
    "    while not done:\n",
    "        # Save memory on last iteration\n",
    "        l = x.shape[-1]\n",
    "        if L - l <= l:\n",
    "            done = True\n",
    "            _x = x[..., :L-l]\n",
    "        else: _x = x\n",
    "\n",
    "        _x = A_ @ _x\n",
    "        x = torch.cat([x, _x], dim=-1) # there might be a more efficient way of ordering axes\n",
    "        if not done: A_ = A_ @ A_\n",
    "\n",
    "    assert x.shape[-1] == L\n",
    "\n",
    "    x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def hippo(N):\n",
    "    \"\"\" Return the HiPPO-LegT state matrices \"\"\"\n",
    "    Q = np.arange(N, dtype=np.float64)\n",
    "    R = (2*Q + 1) ** .5\n",
    "    j, i = np.meshgrid(Q, Q)\n",
    "    A = R[:, None] * np.where(i < j, (-1.)**(i-j), 1) * R[None, :]\n",
    "    B = R[:, None]\n",
    "    A = -A\n",
    "    return A, B\n",
    "\n",
    "class AdaptiveTransition(nn.Module):\n",
    "    \"\"\" General class which supports discretizing a state space equation x' = Ax + Bu\n",
    "    Different subclasses can compute the forward and inverse mults in different ways\n",
    "    This particular method is specialized to the HiPPO-LegT transition for simplicity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, N):\n",
    "        \"\"\"\n",
    "        N: State space order, size of HiPPO matrix\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "\n",
    "        A, B = hippo(N)\n",
    "        A = torch.as_tensor(A, dtype=torch.float)\n",
    "        B = torch.as_tensor(B, dtype=torch.float)[:, 0]\n",
    "        self.register_buffer('A', A)\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "        # Register some common buffers\n",
    "        # (helps make sure every subclass has access to them on the right device)\n",
    "        I = torch.eye(N)\n",
    "        self.register_buffer('I', I)\n",
    "\n",
    "\n",
    "    def forward_mult(self, u, delta):\n",
    "        \"\"\" Computes (I + delta A) u\n",
    "        A: (n, n)\n",
    "        u: (..., n)\n",
    "        delta: (...) or scalar\n",
    "        output: (..., n)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inverse_mult(self, u, delta): # TODO swap u, delta everywhere\n",
    "        \"\"\" Computes (I - d A)^-1 u \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward_diff(self, d, u, v):\n",
    "        \"\"\" Computes the 'forward diff' or Euler update rule: (I - d A)^-1 u + d B v\n",
    "        d: (...)\n",
    "        u: (..., N)\n",
    "        v: (...)\n",
    "        \"\"\"\n",
    "        v = d * v\n",
    "        v = v.unsqueeze(-1) * self.B\n",
    "        x = self.forward_mult(u, d)\n",
    "        x = x + v\n",
    "        return x\n",
    "\n",
    "    def backward_diff(self, d, u, v):\n",
    "        \"\"\" Computes the 'forward diff' or Euler update rule: (I - d A)^-1 u + d (I - d A)^-1 B v\n",
    "        d: (...)\n",
    "        u: (..., N)\n",
    "        v: (...)\n",
    "        \"\"\"\n",
    "        v = d * v\n",
    "        v = v.unsqueeze(-1) * self.B\n",
    "        x = u + v\n",
    "        x = self.inverse_mult(x, d)\n",
    "        return x\n",
    "\n",
    "    def bilinear(self, dt, u, v, alpha=.5):\n",
    "        \"\"\" Computes the bilinear (aka trapezoid or Tustin's) update rule.\n",
    "        (I - d/2 A)^-1 (I + d/2 A) u + d B (I - d/2 A)^-1 B v\n",
    "        dt: (...)\n",
    "        u: (..., N)\n",
    "        v: (...)\n",
    "        \"\"\"\n",
    "        x = self.forward_mult(u, (1-alpha)*dt)\n",
    "        v = dt * v\n",
    "        v = v.unsqueeze(-1) * self.B\n",
    "        x = x + v\n",
    "        x = self.inverse_mult(x, (alpha)*dt)\n",
    "        return x\n",
    "\n",
    "    def gbt_A(self, dt, alpha=.5):\n",
    "        \"\"\" Compute the transition matrices associated with bilinear transform\n",
    "        dt: (...)\n",
    "        returns: (..., N, N)\n",
    "        \"\"\"\n",
    "        # solve (N, ...) parallel problems of size N\n",
    "        dims = len(dt.shape)\n",
    "        I = self.I.view([self.N] + [1]*dims + [self.N])\n",
    "        A = self.bilinear(dt, I, dt.new_zeros(*dt.shape), alpha=alpha) # (N, ..., N)\n",
    "        A = rearrange(A, 'n ... m -> ... m n', n=self.N, m=self.N)\n",
    "        return A\n",
    "\n",
    "    def gbt_B(self, dt, alpha=.5):\n",
    "        B = self.bilinear(dt, dt.new_zeros(*dt.shape, self.N), dt.new_ones(1), alpha=alpha) # (..., N)\n",
    "        return B\n",
    "\n",
    "\n",
    "class LegTTransitionDense(AdaptiveTransition):\n",
    "    \"\"\" Slower and memory inefficient version via manual matrix mult/inv \"\"\"\n",
    "\n",
    "    def forward_mult(self, u, delta, transpose=False):\n",
    "        if isinstance(delta, torch.Tensor):\n",
    "            delta = delta.unsqueeze(-1)\n",
    "        A_ = self.A.transpose(-1, -2) if transpose else self.A\n",
    "        x = (A_ @ u.unsqueeze(-1)).squeeze(-1)\n",
    "        x = u + delta * x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def inverse_mult(self, u, delta, transpose=False):\n",
    "        \"\"\" Computes (I - d A)^-1 u \"\"\"\n",
    "\n",
    "        if isinstance(delta, torch.Tensor):\n",
    "            delta = delta.unsqueeze(-1).unsqueeze(-1)\n",
    "        _A = self.I - delta * self.A\n",
    "        if transpose: _A = _A.transpose(-1, -2)\n",
    "\n",
    "        # x = torch.linalg.solve(_A, u.unsqueeze(-1)).squeeze(-1) # this can run out of memory\n",
    "        xs = []\n",
    "        for _A_, u_ in zip(*torch.broadcast_tensors(_A, u.unsqueeze(-1))):\n",
    "            x_ = torch.linalg.solve(_A_, u_[...,:1]).squeeze(-1)\n",
    "            xs.append(x_)\n",
    "        x = torch.stack(xs, dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class StateSpace(nn.Module):\n",
    "    \"\"\" Computes a state space layer.\n",
    "    Simulates the state space ODE\n",
    "    x' = Ax + Bu\n",
    "    y = Cx + Du\n",
    "    - A single state space computation maps a 1D function u to a 1D function y\n",
    "    - For an input of H features, each feature is independently run through the state space\n",
    "      with a different timescale / sampling rate / discretization step size.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            d, # hidden dimension, also denoted H below\n",
    "            order=-1, # order of the state space, i.e. dimension N of the state x\n",
    "            dt_min=1e-3, # discretization step size - should be roughly inverse to the length of the sequence\n",
    "            dt_max=1e-1,\n",
    "            channels=1, # denoted by M below\n",
    "            dropout=0.0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.H = d\n",
    "        self.N = order if order > 0 else d\n",
    "\n",
    "        # Construct transition\n",
    "        # self.transition = LegTTransition(self.N) # NOTE use this line for speed\n",
    "        self.transition = LegTTransitionDense(self.N)\n",
    "\n",
    "        self.M = channels\n",
    "\n",
    "        self.C = nn.Parameter(torch.randn(self.H, self.M, self.N))\n",
    "        self.D = nn.Parameter(torch.randn(self.H, self.M))\n",
    "\n",
    "        # Initialize timescales\n",
    "        log_dt = torch.rand(self.H) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)\n",
    "        self.register_buffer('dt', torch.exp(log_dt))\n",
    "\n",
    "        # Cached Krylov (convolution filter)\n",
    "        self.k = None\n",
    "\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.output_linear = nn.Linear(self.M * self.H, self.H)\n",
    "\n",
    "\n",
    "    def forward(self, u): # absorbs return_output and transformer src mask\n",
    "        \"\"\"\n",
    "        u: (L, B, H) or (length, batch, hidden)\n",
    "        Returns: (L, B, H)\n",
    "        \"\"\"\n",
    "\n",
    "        # We need to compute the convolution filter if first pass or length changes\n",
    "        if self.k is None or u.shape[0] > self.k.shape[-1]:\n",
    "            A = self.transition.gbt_A(self.dt) # (..., N, N)\n",
    "            B = self.transition.gbt_B(self.dt) # (..., N)\n",
    "            self.k = krylov(u.shape[0], A, B) # (H, N, L)\n",
    "\n",
    "        # Convolution\n",
    "        y = self.linear_system_from_krylov(u, self.k[..., :u.shape[0]]) # (L, B, H, M)\n",
    "\n",
    "        # Dropout\n",
    "        y = self.dropout(self.activation_fn(y))\n",
    "\n",
    "        # Linear\n",
    "        y = rearrange(y, 'l b h m -> l b (h m)') # (L, B, H*M)\n",
    "        y = self.output_linear(y) # (L, B, H)\n",
    "        return y\n",
    "\n",
    "    def linear_system_from_krylov(self, u, k):\n",
    "        \"\"\"\n",
    "        Computes the state-space system y = Cx + Du from Krylov matrix K(A, B)\n",
    "        u: (L, B, ...) ... = H\n",
    "        C: (..., M, N) ... = H\n",
    "        D: (..., M)\n",
    "        k: (..., N, L) Krylov matrix representing b, Ab, A^2b...\n",
    "        y: (L, B, ..., M)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        k = self.C @ k # (..., M, L)\n",
    "\n",
    "        k = rearrange(k, '... m l -> m ... l')\n",
    "        k = k.to(u) # if training in half precision, need to go back to float32 for the fft\n",
    "        k = k.unsqueeze(1) # (M, 1, ..., L)\n",
    "\n",
    "        v = u.unsqueeze(-1).transpose(0, -1) # (1, B, ..., L)\n",
    "        y = triangular_toeplitz_multiply(k, v) # (M, B, ..., L)\n",
    "        y = y.transpose(0, -1) # (L, B, ..., M)\n",
    "        y = y + u.unsqueeze(-1) * self.D.to(device=y.device) # (L, B, ..., M)\n",
    "        # print(y.shape)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 13:29:33.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mOutput shape: torch.Size([256, 32, 256])\u001b[0m\n",
      "\u001b[32m2024-08-08 13:29:33.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mModel info: LSSLModel(\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x StateSpace(\n",
      "      (transition): LegTTransitionDense()\n",
      "      (activation_fn): GELU(approximate='none')\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class LSSLModel(nn.Module):\n",
    "    def __init__(self, num_layers, d, order, dt_min, dt_max, channels, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            StateSpace(d, order, dt_min, dt_max, channels, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(d) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, norm in zip(self.layers, self.norms):\n",
    "            x = x + layer(norm(x))  # Residual connection\n",
    "        return x\n",
    "\n",
    "num_layers = 4  # Example number of layers\n",
    "# d: hidden states, H; order: state space order, N; channels: M\n",
    "model = LSSLModel(num_layers, d=256, order=128, dt_min=1e-3, dt_max=1e-1, channels=1, dropout=0.1)\n",
    "\n",
    "input_tensor = torch.randn(256, 32, 256)  # (sequence_length, batch_size, hidden_dimension)\n",
    "output = model(input_tensor)\n",
    "logger.info(f\"Output shape: {output.shape}\")\n",
    "logger.info(f\"Model info: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 13:29:33.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mNumber of parameters in the model: 397312\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "logger.info(f\"Number of parameters in the model: {num_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lss-cfar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
